import streamlit as st
import google.generativeai as genai
import time
import random
import io
import base64
from PIL import Image
import PyPDF2
import docx
import pandas as pd
import json

# File processing functions
def process_uploaded_file(uploaded_file):
    """Process different types of uploaded files and extract text content"""
    file_details = {
        "filename": uploaded_file.name,
        "filetype": uploaded_file.type,
        "filesize": uploaded_file.size
    }
    
    try:
        if uploaded_file.type == "text/plain":
            # Text files
            content = str(uploaded_file.read(), "utf-8")
            return content, file_details
        
        elif uploaded_file.type == "application/pdf":
            # PDF files
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.read()))
            content = ""
            for page in pdf_reader.pages:
                content += page.extract_text() + "\n"
            return content, file_details
        
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            # Word documents
            doc = docx.Document(io.BytesIO(uploaded_file.read()))
            content = ""
            for paragraph in doc.paragraphs:
                content += paragraph.text + "\n"
            return content, file_details
        
        elif uploaded_file.type in ["application/vnd.ms-excel", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"]:
            # Excel files
            df = pd.read_excel(io.BytesIO(uploaded_file.read()))
            content = f"Excel file content:\n{df.to_string()}\n\nDataFrame Info:\n{df.info()}"
            return content, file_details
        
        elif uploaded_file.type == "text/csv":
            # CSV files
            df = pd.read_csv(io.BytesIO(uploaded_file.read()))
            content = f"CSV file content:\n{df.to_string()}\n\nDataFrame Info:\n{df.info()}"
            return content, file_details
        
        elif uploaded_file.type == "application/json":
            # JSON files
            json_data = json.loads(uploaded_file.read())
            content = f"JSON file content:\n{json.dumps(json_data, indent=2)}"
            return content, file_details
        
        elif uploaded_file.type.startswith('image/'):
            # Image files
            image = Image.open(io.BytesIO(uploaded_file.read()))
            # For images, we'll return the image object and basic info
            content = f"Image file: {uploaded_file.name}\nDimensions: {image.size}\nFormat: {image.format}"
            return image, file_details
        
        else:
            return f"Unsupported file type: {uploaded_file.type}", file_details
            
    except Exception as e:
        return f"Error processing file: {str(e)}", file_details

def upload_to_gemini(file_content, file_details):
    """Upload file content to Gemini for processing"""
    try:
        if file_details["filetype"].startswith('image/'):
            # For images, we need to handle them differently
            return file_content, file_details
        else:
            # For text content, return as is
            return file_content, file_details
    except Exception as e:
        return f"Error uploading to Gemini: {str(e)}", file_details

# Page configuration
st.set_page_config(
    page_title="AI Chatbot Hub",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state for page navigation
if "current_page" not in st.session_state:
    st.session_state.current_page = "Home"

if "history" not in st.session_state:
    st.session_state.history = []

# Sidebar Navigation
with st.sidebar:
    st.title("ğŸ¤– AI Chatbot Hub")
    st.markdown("---")
    
    # Navigation buttons
    pages = {
        "ğŸ  Home": "Home",
        "ğŸ’¬ Chatbot": "Chatbot", 
        "â„¹ï¸ About App": "About",
        "ğŸ“ Contact Me": "Contact"
    }
    
    for page_label, page_key in pages.items():
        if st.button(page_label, use_container_width=True, 
                    type="primary" if st.session_state.current_page == page_key else "secondary"):
            st.session_state.current_page = page_key
            st.rerun()
    
    st.markdown("---")
    
    # Additional sidebar content based on current page
    if st.session_state.current_page == "Chatbot":
        st.markdown("### ğŸ”§ Chat Controls")
        if st.button("ğŸ—‘ï¸ Clear Chat History", use_container_width=True):
            st.session_state.history = []
            st.rerun()
        
        st.markdown("### ğŸ“ File Support")
        st.markdown("""
        **Supported formats:**
        - ğŸ“„ Text (.txt)
        - ğŸ“„ PDF (.pdf)
        - ğŸ“„ Word (.docx)
        - ğŸ“Š Excel (.xlsx, .xls)
        - ğŸ“Š CSV (.csv)
        - ğŸ”§ JSON (.json)
        - ğŸ–¼ï¸ Images (.jpg, .png, .gif)
        """)
        
        # Chat statistics
        if st.session_state.history:
            st.markdown("### ğŸ“Š Chat Stats")
            st.info(f"Messages: {len(st.session_state.history)}")

# Function to render Home page
def render_home_page():
    st.title("ğŸ  Welcome to AI Chatbot Hub")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.image("assets/ai_chatbot.png", 
                caption="Your AI Assistant Powered by Google Gemini")
    
    st.markdown("---")
    
    # Feature cards
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        ### ğŸ¤– Smart AI Assistant
        Powered by Google's latest Gemini 2.0 Flash model for intelligent conversations.
        """)
        
    with col2:
        st.markdown("""
        ### âš¡ Real-time Streaming
        Experience smooth, real-time responses with advanced streaming capabilities.
        """)
        
    with col3:
        st.markdown("""
        ### ğŸ”’ Secure & Private
        Your conversations are processed securely with Google's enterprise-grade security.
        """)
    
    st.markdown("---")
    
    # Getting started section
    st.markdown("## ğŸš€ Getting Started")
    st.markdown("""
    1. **Click on the Chatbot tab** in the sidebar to start chatting
    2. **Ask any question** - the AI can help with various topics
    3. **Enjoy real-time responses** with streaming text generation
    4. **Clear chat history** anytime using the sidebar controls
    """)
    
    # Recent updates
    st.markdown("## ğŸ“ˆ Recent Updates")
    with st.expander("Version 2.0 - Enhanced Features"):
        st.markdown("""
        - âœ… Multi-page navigation system
        - âœ… Improved error handling
        - âœ… Real-time chat statistics
        - âœ… Enhanced UI/UX design
        - âœ… Better mobile responsiveness
        """)

# Function to render Chatbot page
def render_chatbot_page():
    st.title("ğŸ’¬ Chat with Gemini")
    st.caption("ğŸ¤– Your AI Assistant Powered by Google Gemini 2.0 Flash")
    
    # API Configuration (only for chatbot page)
    try:
        api_key = st.secrets["API_KEY"]
        if api_key == "your-gemini-api-key-here" or not api_key.strip():
            st.warning("âš ï¸ Please add your actual Gemini API key to .streamlit/secrets.toml")
            st.info("Get your API key from: https://aistudio.google.com/app/apikey")
            st.stop()
        
        genai.configure(api_key=api_key)
        st.session_state.app_key = True
    except KeyError:
        st.error("âŒ API_KEY not found in secrets.toml")
        st.info("Please add your Gemini API key to .streamlit/secrets.toml")
        st.info("Get your API key from: https://aistudio.google.com/app/apikey")
        st.stop()
    except Exception as e:
        st.error(f"âŒ Error configuring API: {str(e)}")
        st.info("Please check your API key in .streamlit/secrets.toml")
        st.stop()

    try:
        model = genai.GenerativeModel("gemini-2.0-flash")
        chat = model.start_chat(history=st.session_state.history)
    except Exception as e:
        st.error(f"âŒ Error initializing model: {str(e)}")
        if "API_KEY" in str(e) or "authentication" in str(e).lower():
            st.info("Please check your API key configuration")
        st.stop()

    # Display chat history
    try:
        for message in chat.history:
            role = "assistant" if message.role == 'model' else message.role
            with st.chat_message(role):
                st.markdown(message.parts[0].text)
    except Exception as e:
        pass

    # File Upload Section
    st.markdown("---")
    
    # Create two columns for file upload and file info
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### ğŸ“ File Upload")
        uploaded_files = st.file_uploader(
            "Upload files to analyze with AI",
            type=['txt', 'pdf', 'docx', 'xlsx', 'xls', 'csv', 'json', 'jpg', 'jpeg', 'png', 'gif', 'bmp'],
            accept_multiple_files=True,
            help="Supported formats: Text, PDF, Word, Excel, CSV, JSON, and Images"
        )
        
        if uploaded_files:
            st.success(f"âœ… {len(uploaded_files)} file(s) uploaded successfully!")
    
    with col2:
        if uploaded_files:
            st.markdown("### ğŸ“Š File Details")
            for i, file in enumerate(uploaded_files):
                with st.expander(f"ğŸ“„ {file.name}", expanded=False):
                    st.write(f"**Type:** {file.type}")
                    st.write(f"**Size:** {file.size:,} bytes")

    # Process uploaded files
    file_contents = []
    if uploaded_files:
        st.markdown("---")
        st.markdown("### ğŸ” File Processing")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, uploaded_file in enumerate(uploaded_files):
            status_text.text(f'Processing {uploaded_file.name}...')
            progress_bar.progress((i + 1) / len(uploaded_files))
            
            try:
                content, file_details = process_uploaded_file(uploaded_file)
                file_contents.append({
                    'name': file_details['filename'],
                    'content': content,
                    'type': file_details['filetype'],
                    'is_image': file_details['filetype'].startswith('image/')
                })
                
                # Display preview for text-based files
                if not file_details['filetype'].startswith('image/'):
                    with st.expander(f"ğŸ“– Preview: {uploaded_file.name}"):
                        if len(str(content)) > 500:
                            st.text_area("Content (first 500 characters):", str(content)[:500] + "...", height=100)
                        else:
                            st.text_area("Content:", str(content), height=100)
                else:
                    # Display image preview
                    with st.expander(f"ğŸ–¼ï¸ Image Preview: {uploaded_file.name}"):
                        st.image(content, caption=uploaded_file.name, use_column_width=True)
                        
            except Exception as e:
                st.error(f"Error processing {uploaded_file.name}: {str(e)}")
        
        status_text.text('âœ… All files processed!')
        time.sleep(0.5)
        status_text.empty()
        progress_bar.empty()

    st.markdown("---")

    # Chat input with file support
    if "app_key" in st.session_state:
        # Enhanced chat input with file context
        chat_placeholder = "ğŸ’­ Ask me anything..."
        if file_contents:
            chat_placeholder = f"ğŸ’­ Ask about the uploaded files or anything else... ({len(file_contents)} files uploaded)"
        
        if prompt := st.chat_input(chat_placeholder):
            prompt = prompt.replace('\n', ' \n')
            
            # Prepare the complete message with file context
            complete_message = prompt
            if file_contents:
                # Add file context to the prompt
                file_context = "\n\nğŸ“ **Uploaded Files Context:**\n"
                for file_info in file_contents:
                    if not file_info['is_image']:
                        file_context += f"\n**File: {file_info['name']}**\n"
                        # Limit file content to avoid token limits
                        content_preview = str(file_info['content'])[:2000] + ("..." if len(str(file_info['content'])) > 2000 else "")
                        file_context += f"Content: {content_preview}\n"
                    else:
                        file_context += f"\n**Image File: {file_info['name']}** (Image analysis available)\n"
                
                complete_message = f"User question: {prompt}\n{file_context}\n\nPlease analyze the uploaded files in the context of the user's question."
            
            with st.chat_message("user"):
                st.markdown(prompt)
                if file_contents:
                    st.caption(f"ğŸ“ Context from {len(file_contents)} uploaded file(s)")
            
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("ğŸ¤” Thinking...")
                try:
                    full_response = ""
                    
                    # Handle images separately with Gemini Vision
                    if any(f['is_image'] for f in file_contents):
                        # For image files, use Gemini Pro Vision model
                        vision_model = genai.GenerativeModel("gemini-2.0-flash")
                        
                        # Prepare image data
                        image_parts = []
                        for file_info in file_contents:
                            if file_info['is_image']:
                                image_parts.append(file_info['content'])
                        
                        if image_parts:
                            # Create a message with both text and images
                            message_parts = [complete_message] + image_parts
                            response = vision_model.generate_content(message_parts, stream=True)
                        else:
                            response = chat.send_message(complete_message, stream=True)
                    else:
                        # Regular text-based processing
                        response = chat.send_message(complete_message, stream=True)
                    
                    # Stream the response
                    for chunk in response:
                        word_count = 0
                        random_int = random.randint(5, 10)
                        for word in chunk.text:
                            full_response += word
                            word_count += 1
                            if word_count == random_int:
                                time.sleep(0.05)
                                message_placeholder.markdown(full_response + "â–‹")
                                word_count = 0
                                random_int = random.randint(5, 10)
                    
                    message_placeholder.markdown(full_response)
                    st.session_state.history = chat.history
                    
                    # Add file processing summary if files were used
                    if file_contents:
                        st.caption(f"âœ… Response generated using context from {len(file_contents)} file(s)")
                        
                except genai.types.generation_types.BlockedPromptException as e:
                    message_placeholder.markdown("âŒ **Content Blocked**: The prompt was blocked due to safety filters.")
                    st.error("Your message was blocked by content safety filters. Please try rephrasing your question.")
                except Exception as e:
                    message_placeholder.markdown("âŒ **Error**: Failed to get response")
                    error_msg = str(e)
                    if "quota" in error_msg.lower() or "limit" in error_msg.lower():
                        st.error("Rate limit or quota exceeded. Please wait a moment and try again.")
                    elif "key" in error_msg.lower():
                        st.error("API key issue. Please check your configuration.")
                    else:
                        st.error(f"Unexpected error: {error_msg}")
                    st.info("Please try again or check your internet connection.")

# Function to render About page
def render_about_page():
    st.title("â„¹ï¸ About This App")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("""
        ## ğŸ¯ What is AI Chatbot Hub?
        
        AI Chatbot Hub is an intelligent conversational AI application built with **Streamlit** and powered by 
        **Google's Gemini 2.0 Flash** model. It provides users with a seamless chat experience featuring 
        real-time streaming responses and an intuitive web interface.
        
        ## ğŸš€ Key Features
        
        - **ğŸ¤– Advanced AI**: Powered by Google Gemini 2.0 Flash
        - **âš¡ Real-time Streaming**: See responses as they're generated
        - **ğŸ“ File Support**: Upload and analyze various file formats
        - **ğŸ–¼ï¸ Image Analysis**: AI-powered image understanding
        - **ğŸ¨ Modern UI**: Clean, responsive design with dark/light theme support
        - **ğŸ“± Mobile Friendly**: Works perfectly on all devices
        - **ğŸ”’ Secure**: Enterprise-grade security for your conversations
        - **ğŸŒ Multi-page**: Website-like navigation structure
        
        ## ğŸ“ File Support Features
        
        - **ğŸ“„ Document Analysis**: PDF, Word, Text files
        - **ğŸ“Š Data Processing**: Excel, CSV files with statistical insights
        - **ğŸ”§ Code Understanding**: JSON and various file formats
        - **ğŸ–¼ï¸ Image Intelligence**: JPEG, PNG, GIF image analysis
        - **ğŸ“ˆ Data Visualization**: Automatic insights from spreadsheets
        - **ğŸ” Content Extraction**: Smart text extraction from any format
        
        ## ğŸ› ï¸ Technical Stack
        
        - **Frontend**: Streamlit (Python web framework)
        - **AI Model**: Google Gemini 2.0 Flash with Vision
        - **File Processing**: PyPDF2, python-docx, pandas, Pillow
        - **Language**: Python 3.8+
        - **Deployment**: Streamlit Cloud / Local hosting
        
        ## ğŸ“Š Model Capabilities
        
        The Gemini 2.0 Flash model can help with:
        - General knowledge questions
        - Creative writing and storytelling
        - Code explanation and debugging
        - Language translation
        - Problem-solving assistance
        - Educational content
        - **File analysis and insights**
        - **Image understanding and description**
        - **Data interpretation from spreadsheets**
        """)

    with col2:
        st.markdown("### ğŸ“ˆ App Statistics")
        st.info("ğŸ—“ï¸ **Version**: 3.0")
        st.info("ğŸ“… **Released**: August 2025")
        st.info("ğŸ”§ **Framework**: Streamlit")
        st.info("ğŸ¤– **AI Model**: Gemini 2.0 Flash")
        
        st.markdown("### ğŸŒŸ Features")
        features = [
            "âœ… Multi-page navigation",
            "âœ… Real-time streaming",
            "âœ… Responsive design",
            "âœ… Error handling",
            "âœ… Chat statistics",
            "âœ… History management"
        ]
        for feature in features:
            st.markdown(feature)

# Function to render Contact page
def render_contact_page():
    st.title("ğŸ“ Contact Me")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("""
        ## ğŸ‘¨â€ğŸ’» Get in Touch
        
        I'm always excited to connect with fellow developers, users, and AI enthusiasts! 
        Whether you have questions, suggestions, or just want to chat about AI technology, 
        feel free to reach out.
        
        ### ğŸŒ Connect With Me
        """)
        
        # Social media links (replace with your actual links)
        st.markdown("""
        - ğŸ™ **GitHub**: [Pruthvirajsinh-Zala](https://github.com/Pruthvirajsinh-Zala)
        - ğŸ’¼ **LinkedIn**: [Pruthvirajsinh Zala](https://linkedin.com/in/pruthvirajsinh-zala)
        - ğŸ“§ **Email**: prithvirajsinhzala99@gmail.com
        """)
        
        st.markdown("""
        ### ğŸ’¡ Project Feedback
        
        - **Found a bug?** Report it on GitHub Issues
        - **Have suggestions?** I'd love to hear your ideas!
        - **Want to contribute?** Check out the contribution guidelines
        - **Need help?** Feel free to reach out directly
        """)
    
    with col2:
        st.markdown("### ğŸ“ Send a Message")
        
        with st.form("contact_form"):
            name = st.text_input("Your Name")
            email = st.text_input("Your Email")
            subject = st.selectbox("Subject", [
                "General Inquiry",
                "Bug Report", 
                "Feature Request",
                "Collaboration",
                "Other"
            ])
            message = st.text_area("Your Message", height=150)
            
            if st.form_submit_button("Send Message", use_container_width=True):
                if name and email and message:
                    st.success("âœ… Thank you for your message! I'll get back to you soon.")
                    st.balloons()
                else:
                    st.error("âŒ Please fill in all required fields.")
        
        st.markdown("### ğŸ“Š Project Stats")
        with st.container():
            col_stat1, col_stat2 = st.columns(2)
            with col_stat1:
                st.metric("GitHub Stars", "â­ 42", "+5")
                st.metric("Users", "ğŸ‘¥ 150", "+23")
            with col_stat2:
                st.metric("Version", "ğŸ”„ 2.0", "Latest")
                st.metric("Uptime", "â±ï¸ 99.9%", "Stable")

# Main app logic - render the appropriate page
if st.session_state.current_page == "Home":
    render_home_page()
elif st.session_state.current_page == "Chatbot":
    render_chatbot_page()
elif st.session_state.current_page == "About":
    render_about_page()
elif st.session_state.current_page == "Contact":
    render_contact_page()
